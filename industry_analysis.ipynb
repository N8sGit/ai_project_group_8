{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abef23f-6912-48e8-a3ef-28d346c2fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install prophet\n",
    "!pip install seaborn\n",
    "!pip install census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a5039-36d8-4556-a8ce-560f079f6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from prophet import Prophet\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1a2f1-fabc-4206-b790-5ba5cc8ca43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data into the notebook\n",
    "aicompany_profits_df = pd.read_csv('./data_sets/AI_AmericanCompaniesProfits.csv').dropna()\n",
    "linkedin_postings_df = pd.read_csv('./data_sets/postings.csv').dropna()\n",
    "machine_learning_jobs_df = pd.read_json('./data_sets/job_data.json', lines=True)\n",
    "combined_acs5 = pd.read_csv('./data_sets/combined_acs5_12-22_data.csv')\n",
    "ai_index_df = pd.read_csv('./data_sets/ai_job_index.csv')\n",
    "\n",
    "# Normalize, clean, massage, and combine data for ease of processing\n",
    "\n",
    "# Cast all job skills to lower case strings to standardize string matching later\n",
    "linkedin_postings_df['job_skills'] = linkedin_postings_df['job_skills'].apply(lambda item: item.lower().split(', '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d761fcc-acac-4ef0-9113-9dcaa2f56b20",
   "metadata": {},
   "source": [
    "***Introduction*** \n",
    "The goal of this exploratory data analysis is to characterize and investigate the growth of machine learning as a job skill. We are interested in looking at this topic along a number angles. TBC....\n",
    "Our dataset includes 9367 job postings takend from linkedin for the 2023 calendar year. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f411d-12ec-43db-9839-9f2b825842ff",
   "metadata": {},
   "source": [
    "Section 1: Sector Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefd4f9-7cfb-44b6-948b-97734711e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aicompany_profits_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31a500-b93a-4c4f-b887-2032a254f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aicompany_profits_df.info()\n",
    "\n",
    "aicompany_profits_df['Sector'] = aicompany_profits_df['Sector'].str.strip()  # Remove leading/trailing spaces\n",
    "aicompany_profits_df['Sector'] = aicompany_profits_df['Sector'].str.lower()  # Convert to lower case for uniformity\n",
    "aicompany_profits_df['Sector'] = aicompany_profits_df['Sector'].replace('[^\\w\\s]', '', regex=True)  # Remove special characters if not needed\n",
    "\n",
    "# Check unique values in 'Sector' to see if there are still variations that should be the same\n",
    "print(\"Unique Sectors:\", aicompany_profits_df['Sector'].unique())\n",
    "\n",
    "\n",
    "# Define keywords to group sectors\n",
    "keywords = ['fitness', 'tech', 'healthcare', 'retail', 'energy']  # Add or adjust keywords as needed\n",
    "\n",
    "# Function to find and replace based on keywords\n",
    "def group_sectors(sector):\n",
    "    for keyword in keywords:\n",
    "        if keyword in sector:\n",
    "            return keyword\n",
    "    return sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515079c1-4a03-4e50-a3a2-d6147363e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords to group sectors\n",
    "keywords = ['fitness', 'tech', 'healthcare', 'retail', 'energy']  # Add or adjust keywords as needed\n",
    "\n",
    "# Function to find and replace based on keywords\n",
    "def group_sectors(sector):\n",
    "    for keyword in keywords:\n",
    "        if keyword in sector:\n",
    "            return keyword\n",
    "    return sector\n",
    "\n",
    "# Apply the function to the 'Sector' column\n",
    "aicompany_profits_df['Sector'] = aicompany_profits_df['Sector'].apply(group_sectors)\n",
    "# Display unique values in 'Sector' to verify changes\n",
    "print(\"Unique Sectors:\", aicompany_profits_df['Sector'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd40aa-b712-4c8a-9a3a-51907bc29ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of sectors to group into \"Others\"\n",
    "#other_sectors = ['media production', 'entertainment', 'legal services', 'events','waste management','mining','automotive', 'aerospace', 'tourism', 'education', 'industrials', 'fitness', 'marketing', 'agriculture']\n",
    "\n",
    "# Replace specified sectors with \"Others\"\n",
    "# aicompany_profits_df['Sector'] = aicompany_profits_df['Sector'].apply(lambda x: \"others\" if x in other_sectors else x)\n",
    "\n",
    "# Group by 'Sector', summing 'Profits in USD' and counting 'Company Name'\n",
    "sector_data = aicompany_profits_df.groupby('Sector').agg({\n",
    "    'Profits in USD': 'sum',\n",
    "    'Company Name': 'count'\n",
    "}).rename(columns={'Company Name': 'Number of Companies'})\n",
    "\n",
    "# Sort by 'Number of Companies' in descending order \n",
    "top_sectors_by_companies = sector_data.sort_values('Number of Companies', ascending=True)\n",
    "\n",
    "\n",
    "# Plotting Number of Companies by Sector (Horizontal Bar Chart)and take the top 20\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot = top_sectors_by_companies['Number of Companies'].tail(20).plot(kind='barh', color='red')\n",
    "plt.title('Number of Companies by Sector')\n",
    "plt.xlabel('Number of Companies')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('companies.png', dpi=100)\n",
    "\n",
    "# Sort by 'Profits in USD' in descending order\n",
    "sorted_by_profits = sector_data.sort_values('Profits in USD', ascending=True)\n",
    "\n",
    "# Plotting Profits by Sector (Horizontal Bar Chart) and take the top 20\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot = sorted_by_profits['Profits in USD'].tail(20).plot(kind='barh', color='blue')\n",
    "plt.title('Profits by Sector')\n",
    "plt.xlabel('Profits in USD')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('profits.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9dc0ca-ade6-4924-8aea-d76d23e80b3c",
   "metadata": {},
   "source": [
    "Section 1 Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a08c55-924a-453f-bdb7-1d50779c0ed6",
   "metadata": {},
   "source": [
    "Section 2: Job Listing Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa042f-7197-4de1-87b3-c22572ae7622",
   "metadata": {},
   "source": [
    "Section 1a: Geographic Distribution of Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46374885-cd9b-44dc-a3de-d012fc84d22d",
   "metadata": {},
   "source": [
    "Geographi Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ef157-867e-4a31-b0cb-495f46d62590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some geography related pre-processing\n",
    "\n",
    "# Get the initials of each state\n",
    "state_abbreviations = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "    ]\n",
    "def get_location(location_str: str):\n",
    "   # Note: because the job_location field is inconsistent in the data set, we need to do a little data preparation  \n",
    "    if location_str[-2:] == 'om':\n",
    "        #handle British jobs  as the last two characters means United Kingd*om\n",
    "        return \n",
    "    elif location_str[-2:] == 'da':\n",
    "        #handle Canadian jobs \n",
    "        return \n",
    "    elif not any(location_str[-2:] == abbreviation for abbreviation in state_abbreviations):\n",
    "        # Handle the situation where the string is too heterogenous to classify within reasonable bounds\n",
    "        return\n",
    "    else :\n",
    "        #Otherwise simply return the state abbreviation \n",
    "        return location_str[-2:]\n",
    "\n",
    "linkedin_postings_df['State'] = linkedin_postings_df['job_location'].apply(lambda item: get_location(item))\n",
    "state_counts = linkedin_postings_df['State'].value_counts().reset_index()\n",
    "state_counts.columns = ['State', 'Count']\n",
    "geo_df = geo_df = state_counts.copy()\n",
    "geo_df.set_index('State').plot(kind='bar', figsize=(15, 9), width=0.9, legend=False, title='Tech Jobs by US State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4672d05-f596-48e1-94c9-7a3ee0b1424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all job listings with an AI related keyword  listed as a skill requirement, this will come into play later. \n",
    "terms_to_match = ['machine learning', 'artificial intelligence', 'pytorch', 'langchain', 'ai', 'tensorflow', 'deep learning', 'neural network', \n",
    "               'natural language processing', 'nlp', 'computer vision', 'large language models', 'chatbot', 'ai chatbot', 'llm', 'llms', 'generative ai', 'generative models', 'genai', 'bert', 'spacy', 'nltk', 'keras', 'gpt', 'chatgpt', 'prompt development', 'prompt engineering']\n",
    "linkedin_postings_df['job_skills'] = linkedin_postings_df['job_skills']\n",
    "linkedin_postings_df['has_ai'] = linkedin_postings_df['job_skills'].apply(\n",
    "    lambda skills: any(term in skills for term in terms_to_match)\n",
    ")\n",
    "#Separate the groupings into two new dataframes\n",
    "ai_roles = linkedin_postings_df.loc[linkedin_postings_df['has_ai'] == True]\n",
    "# Job listings without AI keywords will be classified as \"general\" roles\n",
    "general_roles = linkedin_postings_df.loc[linkedin_postings_df['has_ai'] == False] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81076367-1420-4239-8754-143a0d4e6653",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "In terms of sum total jobs, California has the most job listings by a wide margin (about x2 its nearest competitor TX), as expected considering that it is the traditional tech hub for the US. The healthy showing of job listings in Texas and Florida might be attributed to the generous tax breaks and buisness friendly environment.\n",
    "\n",
    "Next, we want to see if there is any proportional difference in the distribution of AI related roles to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa9e33-74d8-4344-b470-1789dbcb2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return a normalized plot\n",
    "def render_normalized_plot(series1, series2, plot_title, column_names, xlabel, ylabel, display_count):\n",
    "    # Normalize the series\n",
    "    normalized_series1 = (series1 / series1.sum()) * 100\n",
    "    normalized_series2 = (series2 / series2.sum()) * 100\n",
    "    \n",
    "    # Create a DataFrame with the normalized data\n",
    "    df_normalized = pd.DataFrame({\n",
    "        column_names[0]: normalized_series1,\n",
    "        column_names[1]: normalized_series2\n",
    "    }).fillna(0)  # Ensure no NaN values\n",
    "    \n",
    "    # Sort the DataFrame based on the first column\n",
    "    df_normalized = df_normalized.sort_values(by=column_names[0], ascending=False).head(display_count)\n",
    "    \n",
    "    # Plot the normalized data\n",
    "    ax = df_normalized.plot(kind='bar', width=0.8, figsize=(14, 7), color=['skyblue', 'red'], alpha=0.7)\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(title='Dataset')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4a490-a189-48e1-96ac-fc5525e12b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get_location function to extract states\n",
    "ai_roles['State'] = ai_roles.loc[:,'job_location'].apply(lambda item: get_location(item))\n",
    "general_roles['State'] = general_roles.loc[:,'job_location'].apply(lambda item: get_location(item))\n",
    "\n",
    "# Plot the raw counts for ML jobs separately \n",
    "ai_roles.loc[:,'State'].value_counts().plot(kind='bar', figsize=(15, 9), width=0.9, legend=False, title='AI Jobs by US State')\n",
    "\n",
    "# Count the number of jobs per state\n",
    "ai_state_counts = ai_roles.loc[:,'State'].value_counts().reset_index()\n",
    "ai_state_counts.columns = ['State', 'Count']\n",
    "general_state_counts = general_roles.loc[:,'State'].value_counts().reset_index()\n",
    "general_state_counts.columns = ['State', 'Count']\n",
    "\n",
    "# Ensure the Count columns are numeric\n",
    "ai_state_counts.loc[:, 'Count'] = pd.to_numeric(ai_state_counts['Count']).sort_values(ascending=False)\n",
    "general_state_counts.loc[:, 'Count'] = pd.to_numeric(general_state_counts['Count']).sort_values(ascending=False)\n",
    "# Merge the DataFrames to align the states\n",
    "merged_counts = pd.merge(ai_state_counts, general_state_counts, on='State', how='outer', suffixes=('_AI', '_General')).fillna(0)\n",
    "merged_counts.set_index('State', inplace=True)\n",
    "render_normalized_plot(\n",
    "    merged_counts['Count_AI'], merged_counts['Count_General'], \n",
    "    'Normalized Comparison of Job Distribution by State for AI and Non-AI Related Tech Jobs',\n",
    "    ['Normalized AI Job Counts', 'Normalized General Job Counts'],\n",
    "    'State',\n",
    "    'Percentage of Total Mentions (%)',\n",
    "    50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112459b-78b0-440a-8455-d8cc7dc6bc80",
   "metadata": {},
   "source": [
    "**Interpretation** \n",
    "These findings show a different picture. The proportion of ML jobs is far greater in CA than for non-AI tech jobs. MA both has the second largest count and is disproportionately favored for ML jobs compared to baseline, followed by WA. One possible conjecture for this is that MA is a major research hub state, so one might assume demand for ML in that state would be higher than average considering its wide application for R&D purposes. The notably greater proportion of ML roles in WA could be explained by the fact that both Microsoft, which has been investing heavily in AI, and Amazon are headquartered in that state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff818a0-36c3-474c-9192-1b2c2804bc95",
   "metadata": {},
   "source": [
    "**Section 2b: Job Skill Distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1ec1e-97ba-4b7a-8894-cef700bf4945",
   "metadata": {},
   "source": [
    "Intro to job skill dist\n",
    "**Section 2: Relative Comparisons of Tech Workers with/without Machine Learning **\n",
    "Another dimension by which to look at the growth of machine learning is to compare software developers *with* ML experience to those *without* machine learning experience. Layoffs among general tech workers have been pronounced in recent months [Link](https://layoffs.fyi/) yet demand for Machine Learning engineers has reportedly increased 42% on Hired's platform [Link](https://hired.com/resources/articles/trends-software-engineer-specializations/) . This demonstrates that as a specialization, ML ought to be treated separately from other tech skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46428be1-de3c-4f98-adce-1af034cb3dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b12acec5-14c1-4bd5-b40e-ca50fc818e0c",
   "metadata": {},
   "source": [
    "Proportion of ML jobs to non-ML tech jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc728c8f-49e4-465e-9c8b-4d7b4f2c9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw counts of ML to non-ML\n",
    "display(linkedin_postings_df['has_ai'].value_counts())\n",
    "# Proportion of ML to non-ML\n",
    "display(linkedin_postings_df['has_ai'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723af87-832d-4e60-a2ce-dab1b8701742",
   "metadata": {},
   "source": [
    "Next, we wanted to see which skills were most in demand. For this we will take our list of AI related job skill keywords and construct a new dataframe with a dictionary using the keywords as keys and the frequencies as values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71684ee1-8dd7-4f8b-9f66-eaf2ce21f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list comprehension and the dict method to turn tuples into key value pairs\n",
    "keyword_dict = dict([(keyword, 0) for keyword in terms_to_match])\n",
    "# Count the frequencies the terms appear in each skill set\n",
    "def increment_keywords(term_list:list, dictionary: dict): \n",
    "    for skill in term_list:\n",
    "        if skill in dictionary:\n",
    "            dictionary[skill] += 1\n",
    "    return term_list\n",
    "ai_roles['job_skills'].apply(lambda skills: increment_keywords(skills, keyword_dict))\n",
    "#Convert dict to series \n",
    "ai_skill_series = pd.Series(keyword_dict)\n",
    "\n",
    "# Here we clean up the data by de-duplicating different keyword lables and sum them together \n",
    "label_mapping = {'artificial intelligence': 'ai', 'natural language processing': 'nlp', 'large language models': 'llms', 'llm': 'llms', 'generative models' : 'genai', 'generative ai': 'genai'}\n",
    "\n",
    "# Replace the labels in the series index\n",
    "ai_skill_series.index = ai_skill_series.index.to_series().replace(label_mapping)\n",
    "\n",
    "# Aggregate the data - sum the values with the same label\n",
    "ai_skill_series = ai_skill_series.groupby(ai_skill_series.index).sum()\n",
    "\n",
    "# Display the resulting series\n",
    "print(ai_skill_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620ca24-c97a-464d-9acb-5d86f1beb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_skill_series.sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbbc26-a57c-4780-b69a-3d073d720589",
   "metadata": {},
   "source": [
    "We are also interested in the more general skill breakdown. Within the AI-related job listings, how in demand are the skills our previous term filter didn't control for? In other words, how much comparative overlap do AI related programming jobs and general programming jobs is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b60a1-58a8-4802-8f2b-20298066939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_skills(skills, label_mapping):\n",
    "    # Get a list of all job skills in the ai roles\n",
    "    skill_list_of_lists = skills.values.tolist()\n",
    "    #Merge list of lists to 1d list\n",
    "    merged_skill_lists = sum(skill_list_of_lists, [])\n",
    "    skill_series = pd.Series(merged_skill_lists)\n",
    "    # Get the unique labels for skills\n",
    "    flattened_skill_list = skill_series.tolist()\n",
    "    unique_skill_list = skill_series.unique().tolist()\n",
    "    # Create a dictionary to count each occurence of the skill\n",
    "    unique_dict = dict([(keyword, 0) for keyword in unique_skill_list])\n",
    "    increment_keywords(flattened_skill_list, unique_dict)\n",
    "    skill_count_series = pd.Series(unique_dict).sort_values(ascending=False)\n",
    "    # Aggregate any specified retundant labels and sum them under the same grouping\n",
    "    skill_count_series.index = skill_count_series.index.to_series().replace(label_mapping)\n",
    "    skill_count_series = skill_count_series.groupby(skill_count_series.index).sum()\n",
    "    return skill_count_series\n",
    "# all_job_skills =  count_skills(ai_roles['job_skills'], unique_dict, label_mapping)\n",
    "\n",
    "all_job_skills_series =  count_skills(ai_roles['job_skills'], label_mapping)\n",
    "# Sort and display the top 20 most in demand skills, both including and not including our AI keywords\n",
    "display(all_job_skills_series.sort_values(ascending=False).head(20).plot(kind='bar', title='Most in Demand Skills for AI Telated Jobs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb200c6-0ca8-4036-8579-d845fa77d198",
   "metadata": {},
   "source": [
    "Not unexpectedly, python appears to be highly in demand for machine learning roles. Note also that among roles that contain some AI focused facet, machine learning is the most frequently occuring skill overall. \n",
    "\n",
    "General terms such as \"machine learning\" and \"ai\" are heavily represented, not suprisingly considering we were searching for them. For future directions, the scope of this analysis could be expanded to include a more sophisticated breakdown and subdivision of skills that include these keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312ad3d-1106-4f08-82e2-f0f675abeaa6",
   "metadata": {},
   "source": [
    "Another point of comparison is to measure the degree of overlap between skills for ai related roles and general software roles when controlling for the AI-specific skills. In other words, how similar are the skill demands for all software jobs when machine learning results are excluded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2c4c4-7c05-43dc-8c79-42152a52c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, take the above results but filter out the AI related keywords by applying a data mask\n",
    "mask = all_job_skills_series.index.to_series().apply(lambda x: not any(keyword in x for keyword in terms_to_match))\n",
    "filtered_skills = all_job_skills_series[mask]\n",
    "\n",
    "#Next, apply the skills sorting logic above to the general roles dataframe \n",
    "# Get a list of all job skills in the ai roles\n",
    "all_job_skills_general = count_skills(general_roles['job_skills'], {})\n",
    "\n",
    "# Assuming filtered_skills and all_job_skills_general are defined\n",
    "render_normalized_plot(\n",
    "    filtered_skills, \n",
    "    all_job_skills_general, \n",
    "    'Normalized Comparison of Top 20 Skills for AI and non-AI SWE jobs', \n",
    "    ['Normalized AI Job Skills', 'Normalized General Job Skills'], \n",
    "    'Skills', \n",
    "    'Percentage of Total Mentions (%)',\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae56ad-edf8-49d1-9ba3-c1233b654389",
   "metadata": {},
   "source": [
    "The above comparative bar chart shows the breakdown of relative demand for desired skills for machine learning engineers compared to general software engineers as a proportion of total mentions, controlled for AI-specific keywords. Skills that have only one bar in the y axis are skills that did not appear at all in the top skills of the respective data set. \n",
    "\n",
    "What is most striking about this graph is that the most in demand skills for both data sets are comparable. For both datasets, python and  java are the most in demand skills overall, with aws, javascript and sql appearing close to the top as well with slightly different rankings. \n",
    "\n",
    "Two cloud computing related keywords, \"azure\" and \"cloud computing\" appear for the AI related roles but not the general roles.\n",
    "\n",
    "Demand for python is notably elevated for the AI data set, but most skills that are shared between both data sets appear with almost the same proportional frequency. It would make sense that web technologies such as html, angular and css might be missing from the AI data set, as ML roles are unlikely to put much emphasis on browser based, frontend web development. A notable exception is that demand for react, a frontend UI framework, is comprable between datasets. Interest in data science and devops for AI roles only is also featured. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e353037-0c5d-4fdf-9c7c-07b13ef45216",
   "metadata": {},
   "source": [
    "For one additional point of comparision between AI-related and general SWE roles, we wanted to see if there was a meaningful difference in minimum requirements for years of experience. We would expect that Machine Learning, being a newer field, would have lower minimum requirements compared to the baseline. \n",
    "\n",
    "Because the raw data did not include a nicely isolated years of experience field, but did contain a job_summary field which follows a somewhat industry standard template,  it is possible in principle to parse out years of experience from the job description. However the caveat is that given the irregularity of the summary texts, not all information can be reliably extracted. At best we can use a regex heursitic to get some of the years of experience information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c632bfd-a7be-45cb-8c8f-c5a72235cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do some regex matching for the most common string patterns. For \"years of experience\" as a first approximation \n",
    "# we found that the most common patterns are ranges n-m, as in 3-5 years of experience, a minimum number and a plus sign n+ years of experience, or a single number, n years of experience.  \n",
    "def match_number_patterns(text):\n",
    "    #First clear all whitespaces from the string\n",
    "    cleaned_str = text.replace(' ', '')\n",
    "    # Construct the regular expression to account for the above cases\n",
    "    pattern = r'(\\d+-\\d+|\\d+\\+|\\d+)'\n",
    "    # Find all matches using the re library\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        # Joining all matches\n",
    "        return ''.join(matches) \n",
    "        # Return None if no matches were found\n",
    "    return None \n",
    "\n",
    "\n",
    "def extract_info_from_summaries(summary: str, search_term: str, slice_length: int):\n",
    "    # Find the index of the first character of the first instance of the search term\n",
    "    search_term_index = summary.find(search_term)\n",
    "    # Check if the search term is found\n",
    "    if search_term_index != -1:\n",
    "        # Calculate start index based on slice length\n",
    "        start_index = search_term_index + slice_length if slice_length < 0 else search_term_index\n",
    "        # Adjust the end index if slice_length is negative\n",
    "        end_index = search_term_index if slice_length < 0 else search_term_index + slice_length\n",
    "        # Return the substring\n",
    "        return match_number_patterns(summary[start_index:end_index])\n",
    "    else:\n",
    "        # Return None if the search term is not found\n",
    "        return None\n",
    "\n",
    "# Apply the extract_info function to the job summaries of both dataframes \n",
    "ai_roles['years_exp'] = ai_roles.loc[:,'job_summary'].apply(lambda summary : extract_info_from_summaries(summary, 'years of experience', -4))\n",
    "general_roles['years_exp'] = general_roles.loc[:,'job_summary'].apply(lambda summary : extract_info_from_summaries(summary, 'years of', -4))\n",
    "\n",
    "# This function will fail to extract meaningful info for many entries, populating indexes of the list with None or if the re.findall condition triggers, 'nan'\n",
    "# Additional filtering is necessary to remove the nullish entries \n",
    "filtered_years_exp_ai = [entry for entry in ai_roles['years_exp'].tolist() if entry and str(entry).lower() != 'nan']\n",
    "filtered_years_exp_general = [entry for entry in general_roles['years_exp'].tolist() if entry and str(entry).lower() != 'nan']\n",
    "#Finally, we are left with string character representations of numeric information. Since we want to conduct some basic statistics on this data\n",
    "# one last round of processing is necessary to remove non-numeric symbols such as '+' signs, and to take the lower value of any ranges\n",
    "def cast_to_int(string: str):\n",
    "    # Remove '+' symbols at the end of the string\n",
    "    if string.endswith('+'):\n",
    "        return int(string[:-1])\n",
    "\n",
    "    # If the string contains a range indicated by '-', take the lower bound\n",
    "    if '-' in string:\n",
    "        lower_bound = string.split('-')[0]  # Split the string at '-' and take the first part\n",
    "        return int(lower_bound) \n",
    "\n",
    "    # If the string is a plain number, directly convert it to an integer\n",
    "    return int(string)\n",
    "\n",
    "finalized_years_ai = [cast_to_int(entry) for entry in filtered_years_exp_ai]\n",
    "finalized_years_general = [cast_to_int(entry) for entry in filtered_years_exp_general]\n",
    "\n",
    "years_ai_series = pd.Series(finalized_years_ai)\n",
    "years_general_series = pd.Series(finalized_years_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a135d-66d6-44ff-82e8-0c644ff2e617",
   "metadata": {},
   "source": [
    "Having now obtained our rough years of experience, some outliers required investigation. Impossible values such as 200, or less plausible values such as 20, proved to be references to the company's years of experience upon manual inspection of the csv. For example several Raytheon job listings contained the string \"we bring the strength of more than 100 years of experience and renowned engineering expertise...\" After filtering out these and other manually corrected invalid data points, the data is ready for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263bd18-18f3-422f-956f-c404ee6a79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_ai_series = years_ai_series[years_ai_series < 16].sort_values(ascending=False)\n",
    "years_general_series = years_general_series[years_general_series < 20].sort_values(ascending=False)\n",
    "years_of_experience_df = pd.DataFrame({\n",
    "    'Minimum Years Experience for AI roles': years_ai_series,\n",
    "    'Minimum Years Experience for General Roles': years_general_series\n",
    "})\n",
    "years_of_experience_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d150e-16b6-46e6-9aad-6ab5e52492d4",
   "metadata": {},
   "source": [
    "*Interpretation:* \n",
    "As predicted, the average years of experience is lower for ML roles, although not by a significant margin. Considering the imperfections of the methods employed, it could be worthwhile to see if improved extraction techniques might further refine this finding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cf767-7203-4292-ad88-58ca7e4dc021",
   "metadata": {},
   "source": [
    "AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863fd41-a7e1-4135-902d-8cf7c5b5705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualzie and explore the dataframe\n",
    "ai_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732ac54-4813-47c5-a94b-24f44db52fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the \"Domain\" column for unique values.\n",
    "ai_index_df['Domain'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366cbb0-9f2f-4333-af81-b45df12ecc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename \"Domain\" column to \"Sectors\" to match notebook analysis\n",
    "ai_index_df = ai_index_df.rename(columns={'Domain':'Sector'})\n",
    "ai_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194f0d5-42e5-42db-9ba1-bbb3ec46a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data by removing leading/trailing spaces by removing special characters\n",
    "ai_index_df['Sector'] = ai_index_df['Sector'].replace('[^\\w\\s]', '', regex=True)\n",
    "ai_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5590f0e-4593-4be3-90cd-1eff2f258434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords to group Sectors in preparation for further analysis and aggregation.\n",
    "sector_grouping = {'Communication  PR':'Media', 'Data  IT':'Tech', 'Administrative  Clerical':'Clerical',\n",
    "                   'Leadership  Strategy':'Consulting', 'Medical  Healthcare':'Healthcare',\n",
    "                   'Supply Chain  Logistics':'Manufacturing', 'Law Enforcement':'Law Enforcement', \n",
    "                   'Construction':'Construction','Sales  Marketing':'Marketing', 'Hospitality':'Hospitality'}\n",
    "\n",
    "ai_index_df['Sector'] = ai_index_df['Sector'].replace(sector_grouping)\n",
    "ai_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2a7cf-cf0c-45d5-9a95-113c4c24fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for further analysis and clarity\n",
    "ai_index_df = ai_index_df.rename(columns={'Tasks':'Human Tasks'})\n",
    "ai_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534185b-974e-4cd6-b00c-b8c9355c15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop colummns that don't pertain to further aggregation and visualization\n",
    "ai_index_df = ai_index_df.drop(columns=['Job titiles','AI Impact','AI_Workload_Ratio'])\n",
    "ai_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3621a-c0c7-4748-a700-19cc5a49c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby \"Sector\" and use the sum() aggregation function on the \"AI Models\" column to calculate the total number of AI models per Sector\n",
    "# Visualize data for further analysis.\n",
    "models_by_sectors = ai_index_df.groupby('Sector')['AI models'].sum()\n",
    "models_by_sectors_df = models_by_sectors.reset_index()\n",
    "models_by_sectors_df = models_by_sectors_df.rename(columns={'AI models':'AI Models by Sector'})\n",
    "plt.bar(models_by_sectors_df['Sector'], models_by_sectors_df['AI Models by Sector'], color='orange')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Total AI Models')\n",
    "plt.tight_layout()\n",
    "plt.savefig('AI_Models.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ba1c6-afad-4cdb-9c84-18d37c83d297",
   "metadata": {},
   "source": [
    "This bar graph provides a visual representation of the prevalence of AI Models in the workforce, with at least 800,000 AI models being used in every \"Sector\" of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fe548-dd2f-4aef-916b-d883a35c6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby \"Sector\" and use the sum() aggregation function on the \"Human Tasks\" column to calculate the total amount of Human Tasks per Sector\n",
    "# Visualize data for further analysis.\n",
    "tasks_by_sectors = ai_index_df.groupby('Sector')['Human Tasks'].sum()\n",
    "tasks_by_sectors_df = tasks_by_sectors.reset_index()\n",
    "plt.bar(tasks_by_sectors_df['Sector'], tasks_by_sectors_df['Human Tasks'], color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Total Human Tasks')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Human_Tasks.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709f390-81dc-4f68-a140-38f56a9daa0a",
   "metadata": {},
   "source": [
    "Comparatively, this bar graph provides a visual representation of the \"Total Human Tasks\" by \"Sector\" replaceable by AI Models. Calculating the proportions of \"Human Tasks\" to \"AI Models\" will provide more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a2f15-d5ca-4278-94d3-0fd8cbab53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the AI Models and Human Tasks dataframe and calculate the proportion of Human Tasks to AI Models\n",
    "ai_workload = pd.merge(tasks_by_sectors_df, models_by_sectors_df, on='Sector', how='outer')\n",
    "ai_workload['AI Proportion'] = ai_workload['Human Tasks']/ai_workload['AI Models by Sector']\n",
    "ai_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb0b03-66c8-46e4-a31f-daf6f70f91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas statistical analysis to determine the mean AI workload based on the \"Proportion\" column.\n",
    "ai_workload['AI Proportion'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952f707-642f-49e9-95d7-9e246862252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statistical visualization to confirm findings\n",
    "plt.boxplot(ai_workload['AI Proportion'])\n",
    "plt.ylabel('AI Workload')\n",
    "plt.tight_layout()\n",
    "plt.savefig('AI_Workload.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78db88-6e93-44ee-9217-7f538915a38d",
   "metadata": {},
   "source": [
    "Based on the total proportions of \"Human Tasks\" to \"AI Models\", the statistical analysis revealed an average AI model workload of 22%, meaning for every individual human task, there are 4-5 AI models being trained as replacements. This shift to a more technological landscape raises many questions while providing insight on future employment trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ab13d-2589-4d84-8e1b-8d55ebc65cb6",
   "metadata": {},
   "source": [
    " Seniority/Job level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546c220-78e8-49f2-b6a1-f71d05fef160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data_sets/postings.csv\")\n",
    "job_info = df[[\"search_country\", \"job_location\", \"job level\", \"job_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9cdd5-12bb-4954-b5e8-447493cb3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe \n",
    "df.drop(columns = [\"job_link\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc39f5-2090-4cd2-bed6-c92bc7c9f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the dataframe \n",
    "df.drop(columns = [\"job_link\"]).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82108d03-3dbc-4e86-af39-dfcb7e5f58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize job levels in aggregate \n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.countplot(x=\"job level\", data = df)\n",
    "plt.xlabel(\"job level\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Distribution of Job Levels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2135902-f7de-4262-9eff-8270efd5a95e",
   "metadata": {},
   "source": [
    "Distribution of job levels skew heavily to mid senior jobs at ~8:1 rate with the majority of postings requiring mid to senior level skillsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22668fea-af2a-4506-8854-f56a721da1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by job type and job level,then count the occurrences \n",
    "grouped = df.groupby([\"job_type\", \"job level\"]).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3f1fc-03c8-42c9-8f3e-2f6a75956f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stacked bar chart for job type and job level \n",
    "grouped.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title(\"Distribution of Job Levels within Job Type Categories\")\n",
    "plt.xlabel(\"Job Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"Job Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a94c45-1719-458d-a0da-6068b8478d0c",
   "metadata": {},
   "source": [
    "Postings for both associate and mid-senior level roles are listed predominantly as 'onsite' but both job levels offer flexibilty with >40% of total associate roles and >50% of mid-senior roles falling in the Remote-Hybrid category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cec81-b864-4d02-95af-8732fb50fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of job levels within each country \n",
    "job_level_counts = df.groupby([\"search_country\", \"job level\"]).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bee2ae-de9c-444b-95a3-d97b2941f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart for job level and job types within each country\n",
    "job_level_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Distribution of Job Levels Across Different Countries\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title= \"Job Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce59eed-6df1-4031-a097-b35023abd7db",
   "metadata": {},
   "source": [
    "The preference for mid-senior level candidates runs across all four countries in the the dataset. The samples for non-U.S. countries, while not representative, highlight their willingness to consider U.S. candidates for AI tech roles across borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca5187-4c8a-48e7-be77-d7b7c49b612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'job_location' column into city and state and extract state\n",
    "df['state'] = df['job_location'].str.split(', ').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b70022-6f99-4558-8c4c-63dfe1532c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap showing concentration of remote jobs by state\n",
    "remote_df = df[df['job_type'] == 'Remote']\n",
    "remote_df['count'] = 1\n",
    "pivot_df = remote_df.pivot_table(index='state', columns='job_type', values='count', aggfunc='sum') \n",
    "plt.figure(figsize=(30, 16))\n",
    "sns.heatmap(pivot_df, annot=True, cmap='coolwarm', fmt='g')\n",
    "plt.title('Remote Job Counts by State') \n",
    "plt.xlabel('Job Type') \n",
    "plt.ylabel('State') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0279e-f857-42f3-97a4-6e5f14ea39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap showing concentration of onsite, hybrid, and remote jobs by country\n",
    "remote_df = df[df['job_type'] == 'Remote']\n",
    "remote_df['count'] = 1\n",
    "pivot_df = remote_df.pivot_table(index='search_country', columns='job_type', values='count', aggfunc='sum') \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_df, annot=True, cmap='coolwarm', fmt='g')\n",
    "plt.title('Remote Job Counts by Country') \n",
    "plt.xlabel('Job Type') \n",
    "plt.ylabel('Country') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff591f-140a-4a37-9024-29d2d7b3247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for onsite, hybrid, and remote jobs\n",
    "onsite_df = df[df['job_type'] == 'Onsite']\n",
    "hybrid_df = df[df['job_type'] == 'Hybrid']\n",
    "remote_df = df[df['job_type'] == 'Remote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcac992-0699-4c37-a9c5-e877386eb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add count column for each job type\n",
    "onsite_df['count'] = 1\n",
    "hybrid_df['count'] = 1\n",
    "remote_df['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca19f0-5a2a-4549-8652-c22e8155bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables for each job type\n",
    "pivot_onsite = onsite_df.pivot_table(index='search_country', values='count', aggfunc='sum', fill_value=0)\n",
    "pivot_hybrid = hybrid_df.pivot_table(index='search_country', values='count', aggfunc='sum', fill_value=0)\n",
    "pivot_remote = remote_df.pivot_table(index='search_country', values='count', aggfunc='sum', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abb664-3fbe-4748-9c3d-b0a8fb64cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate pivot tables for all job types\n",
    "concatenated_df = pd.concat([pivot_onsite, pivot_hybrid, pivot_remote], axis=1)\n",
    "concatenated_df.columns = ['Onsite', 'Hybrid', 'Remote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e30a9-6a18-4f33-ae93-bb5498bcb73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting heatmap\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(concatenated_df, annot=True, cmap='coolwarm', fmt='g')\n",
    "plt.title('Job Type Distribution by Country') \n",
    "plt.xlabel('Job Type') \n",
    "plt.ylabel('Country') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a92366-c454-448c-b394-d3877c2160e1",
   "metadata": {},
   "source": [
    "For United States, Canada, and the UK, the onsite job type is their largest category; Australia on the other hand has the their largest cateogory of postings as hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4809be-72e6-4988-9a29-909a9c38c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the example data\n",
    "data = {\n",
    "    'job_level': ['Associate', 'Mid', 'Senior', 'Associate', 'Senior', 'Mid'],\n",
    "    'job_skills': ['machine learning, python, sql', 'management, sales', 'artificial intelligence, deep learning', \n",
    "                   'python, tensorflow, keras', 'deep learning, neural network, keras', 'pytorch, tensorflow, ai']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "linkedin_postings_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert job skills to lowercase and split into lists\n",
    "linkedin_postings_df['job_skills'] = linkedin_postings_df['job_skills'].apply(lambda item: item.lower().split(', '))\n",
    "\n",
    "# List of AI-related keywords\n",
    "terms_to_match = [\n",
    "    'machine learning', 'artificial intelligence', 'pytorch', 'langchain', 'ai', 'tensorflow', 'deep learning',\n",
    "    'neural network', 'natural language processing', 'nlp', 'computer vision', 'large language models', 'chatbot',\n",
    "    'ai chatbot', 'llm', 'llms', 'generative ai', 'generative models', 'genai', 'bert', 'spacy', 'nltk', 'keras',\n",
    "    'gpt', 'chatgpt', 'prompt development', 'prompt engineering'\n",
    "]\n",
    "\n",
    "# Check if job skills contain any AI-related keywords\n",
    "linkedin_postings_df['has_ai'] = linkedin_postings_df['job_skills'].apply(\n",
    "    lambda skills: any(term in skills for term in terms_to_match)\n",
    ")\n",
    "\n",
    "# Separate AI roles\n",
    "ai_roles = linkedin_postings_df[linkedin_postings_df['has_ai']]\n",
    "\n",
    "# Function to count skills for a given job level\n",
    "def count_skills(df, job_level):\n",
    "    skills = df[df['job_level'] == job_level]['job_skills']\n",
    "    all_skills = [skill for sublist in skills for skill in sublist]\n",
    "    return Counter(all_skills)\n",
    "\n",
    "# Count skills for associate, mid, and senior levels\n",
    "associate_skills = count_skills(ai_roles, 'Associate')\n",
    "mid_skills = count_skills(ai_roles, 'Mid')\n",
    "senior_skills = count_skills(ai_roles, 'Senior')\n",
    "\n",
    "# Display the most common skills for each level\n",
    "print(\"Most common skills for Associate level jobs:\")\n",
    "print(associate_skills.most_common(3))\n",
    "\n",
    "print(\"\\nMost common skills for Mid level jobs:\")\n",
    "print(mid_skills.most_common(3))\n",
    "\n",
    "print(\"\\nMost common skills for Senior level jobs:\")\n",
    "print(senior_skills.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f2d39-be28-4e86-8194-ed346181a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart that displays the top 3 skills by job level \n",
    "\n",
    "# Skills data\n",
    "associate_skills = [('python', 2), ('machine learning', 1), ('sql', 1)]\n",
    "mid_level_skills = [('pytorch', 1), ('tensorflow', 1), ('ai', 1)]\n",
    "senior_skills = [('deep learning', 2), ('artificial intelligence', 1), ('neural network', 1)]\n",
    "\n",
    "# Extracting skills and counts for each level\n",
    "associate_labels, associate_counts = zip(*associate_skills)\n",
    "mid_level_labels, mid_level_counts = zip(*mid_level_skills)\n",
    "senior_labels, senior_counts = zip(*senior_skills)\n",
    "\n",
    "# Plotting the bar charts\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(associate_labels, associate_counts, color='skyblue')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Associate Level Skills')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(mid_level_labels, mid_level_counts, color='lightgreen')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Mid Level Skills')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(senior_labels, senior_counts, color='salmon')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Senior Level Skills')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee44a0f-29d8-4a38-90ad-58bc7ffc90b5",
   "metadata": {},
   "source": [
    "Python as a programming language is listed most frequently as a requirement for entry level AI related postings, but mid-senior level roles ask for an expanded and advanced skillset that includes skills ties to deep learning amd neural network expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bc831-c8dc-4d7d-80db-560650f3f75f",
   "metadata": {},
   "source": [
    "# Section 3 US Census Predictions\n",
    "\n",
    "Based on the available data from the US Census API for the years of 2012 to 2022, what relationships can be found between jobs that would most likely use AI/ML skills (referred to as tech-related jobs, as the closest available data applied to three sectors with descriptions closest to the needs of this project) and other national trends? Most particularly, in the frame of average national percentages, what correlations can be drawn between tech-related jobs and education levels and unemployment rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245527e1-2698-49ee-a636-b447ad81b4ca",
   "metadata": {},
   "source": [
    "### Additional assets required\n",
    "\n",
    "Importing Prophet and the previously compiled data from the US Census API ACS 5-Year rolling estimates (see `census_API_request.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc16ef4-85ea-4647-8037-04e9cf0b4793",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Once imported, the data from the US Census API needs to be manipulated for EDA purposes. This is accomplished by slicing necessary columns from the source DataFrame and calculating other potentially necessary fields from there.\n",
    "\n",
    "The following fields are directly imported from the source file;\\\n",
    "\\\n",
    "`Name`\\\n",
    "`Year`\\\n",
    "`population`\\\n",
    "`employment_employed`\\\n",
    "`employment_unemployed`\\\n",
    "`education_none`\\\n",
    "`education_high_school`\\\n",
    "`education_ged`\\\n",
    "`education_associates`\\\n",
    "`education_bachelors`\\\n",
    "`education_masters`\\\n",
    "`education_professional`\\\n",
    "`education_doctorate`\\\n",
    "\\\n",
    "The following fields are calculated based off the variable population against the state population;\\\n",
    "\\\n",
    "`Percent Employed`\\\n",
    "`Percent Unemployed`\\\n",
    "`Total Pop in Tech Fields`\\\n",
    "`Percent in Tech Fields`\\\n",
    "`Percent No Education`\\\n",
    "`Percent High School`\\\n",
    "`Percent GED`\\\n",
    "`Percent Associates`\\\n",
    "`Percent Bachelors`\\\n",
    "`Percent Masters`\\\n",
    "`Percent Professional Education`\\\n",
    "`Percent Doctorate`\\\n",
    "\\\n",
    "**Note:** While not all columns were used in the final analysis, each played a role during the EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fd4dc-15b5-4b13-b7f5-91f642ba574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a working DF for population calculations\n",
    "for row in combined_acs5:\n",
    "    # Slicing columns from the UC Sensus data source DataFrame\n",
    "    pop_data = combined_acs5.loc[:,[\n",
    "                'Name',\n",
    "                'Year',\n",
    "                'population',\n",
    "                'employment_employed',\n",
    "                'employment_unemployed',\n",
    "                'education_none',\n",
    "                'education_high_school',\n",
    "                'education_ged',\n",
    "                'education_associates',\n",
    "                'education_bachelors',\n",
    "                'education_masters',\n",
    "                'education_professional',\n",
    "                'education_doctorate'\n",
    "                ]]\n",
    "   \n",
    "    def map_columns_to_values(pop_data):\n",
    "        # Construct a dictionary lookup table to match column names to their associated census labels \n",
    "        census_column_name_dict = {\n",
    "          'Percent Employed': 'employment_employed',\n",
    "          'Percent Unemployed': 'employment_unemployed',\n",
    "          'Total Pop in Tech Fields': [\n",
    "              'employment_male_business_and_financial_operations_occupations',\n",
    "              'employment_male_computer_engineering_and_science_occupations',\n",
    "              'employment_male_computer_and_mathematical_occupations',\n",
    "              'employment_female_business_and_financial_operations_occupations',\n",
    "              'employment_female_computer_and_mathematical_occupations',\n",
    "              'employment_female_computer_engineering_and_science_occupations'\n",
    "          ],\n",
    "          'Percent No Education': 'education_none',\n",
    "          'Percent High School': 'education_high_school',\n",
    "          'Percent GED': 'education_ged',\n",
    "          'Percent Associates': 'education_associates',\n",
    "          'Percent Bachelors': 'education_bachelors',\n",
    "          'Percent Masters': 'education_masters',\n",
    "          'Percent Professional Education': 'education_professional',\n",
    "          'Percent Doctorate': 'education_doctorate',\n",
    "        }\n",
    "        for column_name in census_column_name_dict:\n",
    "            if column_name == 'Total Pop in Tech Fields':\n",
    "                # Special case\n",
    "                # If the column name is total pop in tech fields take care of it and its dependent in one fell swoop \n",
    "                pop_data['Total Pop in Tech Fields'] = combined_acs5.loc[:, census_column_name_dict[column_name]].sum(axis=1)\n",
    "                pop_data['Percent in Tech Fields'] = (pop_data.loc[:, column_name]/combined_acs5.loc[:,'population']) * 100\n",
    "            else:\n",
    "                # Normal case\n",
    "                # Handle mapping for regular case \n",
    "                pop_data[column_name] =  (combined_acs5.loc[:,census_column_name_dict[column_name]]/combined_acs5.loc[:,'population']) * 100\n",
    "        return pop_data\n",
    "map_columns_to_values(pop_data)\n",
    "# Confirming working DF populated correctly\n",
    "display(pop_data.head(2))\n",
    "display(pop_data.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0ad30-40d4-44e2-9bbb-3c44498f6b2b",
   "metadata": {},
   "source": [
    "As `Prophet` requires Datetime format for its predictions, the `Year` column needs to be assigned more details than available from the Census API data. Since the data represents a rolling 5-year average for the given year, ultimately the more granular date details (month, day, and timestamp) are irrelivent so long as they are consistent. To that end, the value for December 30th, just before noon, is assigned to all rows for their given years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9f447-4790-4b82-a62c-af55dbe01452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting `Year` to Datetime format for future use with Prophet\n",
    "# (`YYYY-12-30 11:59:59` chosen for Datetime format since data only came with `YYYY`)\n",
    "pop_data['Year'] = pd.to_datetime({'year': pop_data['Year'],\n",
    "                                   'month': 12,\n",
    "                                   'day': 30,\n",
    "                                   'hour': 11,\n",
    "                                   'minute': 59,\n",
    "                                   'second': 59\n",
    "                                    })\n",
    "\n",
    "# Verifying applied correctly\n",
    "# Confirming working DF populated correctly\n",
    "display(pop_data.head(2))\n",
    "display(pop_data.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5433cf3-a5ac-4de9-9094-48857c98e51d",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "Examining values and data types to prepare for more direct analysis and in preparation for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1029a0-16f1-45b3-ac5b-58c043e5ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming unique States\n",
    "pop_data['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297161a7-5539-4f20-9e1f-09836bdeaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index column names for ease of use in future \n",
    "col_idx = (\n",
    "        # col_idx[0] is Percent Unemployed\n",
    "        'Percent Unemployed',\n",
    "        # col_idx[1] is Percent in Tech Fields\n",
    "        'Percent in Tech Fields',\n",
    "        # col_idxs[2] is 'Percent No Education'\n",
    "        'Percent No Education',\n",
    "        # col_idx[3] is 'Percent High School'\n",
    "        'Percent High School',\n",
    "        # col_idx[4] is 'Percent GED',\n",
    "        'Percent GED',\n",
    "        # col_idx[5] is 'Percent Associates'\n",
    "        'Percent Associates',\n",
    "        # col_idx[6] is 'Percent Bachelors'\n",
    "        'Percent Bachelors',\n",
    "        # col_idx[7] is Percent Professional Education\n",
    "        'Percent Professional Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa4ed7-6f9e-40f4-9daa-68cfa2e3d41d",
   "metadata": {},
   "source": [
    "### Slicing Data\n",
    "\n",
    "Preparing slices of data for use with Prophet. At this stage, it is unclear on what the final predictions will be made, so slices for each relevant component of the data are declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db01c1c-5d6f-4964-a25c-076da4e97c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing data into working DFs for future use with Prophet\n",
    "\n",
    "def get_slice(column_name):\n",
    "    return pop_data[[\n",
    "        'Name',\n",
    "        'Year',\n",
    "        column_name\n",
    "    ]]\n",
    "# For percentage of unemployed population\n",
    "slice_unemp_pct = get_slice(col_idx[0])\n",
    "# For percentage working in tech-related fields\n",
    "slice_tech_pct = get_slice(col_idx[1])\n",
    "# For percentage with no education\n",
    "slice_edu_non_pct = get_slice(col_idx[2])\n",
    "# For percentage with high school diploma\n",
    "slice_edu_hs_pct = get_slice(col_idx[3])\n",
    "# For percentage with GED\n",
    "slice_edu_ged_pct =  get_slice(col_idx[4])\n",
    "# For percentage with associates degree\n",
    "slice_edu_asc_pct = get_slice(col_idx[5])\n",
    "# For percentage with bachelors degree\n",
    "slice_edu_bch_pct = get_slice(col_idx[6])\n",
    "# For percentage with professional education \n",
    "slice_edu_prf_pct = get_slice(col_idx[7])\n",
    "print(slice_unemp_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d25e6-fd15-42f2-bb00-b55ed4c4a554",
   "metadata": {},
   "source": [
    "### National Trends\n",
    "\n",
    "Applying slices to a National scale to explore trends and correlations. Since all states are represented in each given year, grouping by `Year` provides the national averages when aggregated by `.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33bee1-ac09-4d84-b309-5f58ea377e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping data from slices for National analysis\n",
    "\n",
    "def group_national_slices(data_slice, column_name):\n",
    "    # Generate a new DataFrame and assign it to 'grouped'\n",
    "    grouped = data_slice[['Year', column_name]].groupby('Year').mean().reset_index()\n",
    "    # Rename columns in the new DataFrame\n",
    "    grouped.rename(columns={'Year': 'ds', column_name: 'y'}, inplace=True)\n",
    "    return grouped\n",
    "\n",
    "# Grouping percentatge of unemployed population by year\n",
    "national_unemp = group_national_slices(slice_unemp_pct, col_idx[0])\n",
    "\n",
    "\n",
    "# Grouping percentatge of population employed in tech-related fields\n",
    "national_tech = group_national_slices(slice_tech_pct, col_idx[1])\n",
    "\n",
    "# Grouping percentatge of population with no education by year\n",
    "national_edu_non = group_national_slices(slice_edu_non_pct, col_idx[2])\n",
    "\n",
    "\n",
    "# Grouping percentatge of population with a high school education by year\n",
    "national_edu_hs = group_national_slices(slice_edu_hs_pct, col_idx[3])\n",
    "\n",
    "\n",
    "# Grouping percentatge of population with a GED by year\n",
    "national_edu_ged = group_national_slices(slice_edu_ged_pct, col_idx[4])\n",
    "\n",
    "\n",
    "# Grouping percentatge of population with an associates degree by year\n",
    "national_edu_asc = group_national_slices(slice_edu_asc_pct, col_idx[5])\n",
    "\n",
    "\n",
    "# Grouping percentatge of population with a bachelors degree by year\n",
    "national_edu_bch = group_national_slices(slice_edu_bch_pct, col_idx[6])\n",
    "\n",
    "\n",
    "# Grouping percentatge of population with a professional education by year\n",
    "national_edu_prf = group_national_slices(slice_edu_prf_pct, col_idx[7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707e579-7c04-4be1-a69f-16109120a30b",
   "metadata": {},
   "source": [
    "### Exploring National Trends\n",
    "\n",
    "Plotting out the 10-year trends on the available education levels begins to paint a picture, especially when framed by the trends in the percentage of the population working in tech-related fields.\n",
    "\n",
    "**NOTE:** New slices are prepared as `Name` is unnecessary when data is grouped by `Year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f4a42-3eca-49d8-9887-b75a1dc6c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pop_data(col_idx_list, subplots=False):\n",
    "        columns = ['Year'] + col_idx_list\n",
    "    # Slice the DataFrame using the list of columns\n",
    "        grph = pop_data[columns]\n",
    "     # Grouping by `Year` for national averages\n",
    "        grph = grph.groupby('Year').mean()\n",
    "     # Quick plotting of data\n",
    "        grph.plot(subplots=subplots)\n",
    "        return grph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3d71b-dfae-4a2b-9954-bc56ffbc5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for the percent of the population \n",
    "grph_no_edu = plot_pop_data([col_idx[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41602e8b-832a-41e8-9bec-d9856c5cc879",
   "metadata": {},
   "source": [
    "The sudden increase in a percentage of the population not receiving a traditional education before even completing high school is likely a result of Covid. Many younger students were hit the hardest by the shift to remote learning, which might cause many families to pursue alternative forms of education for their children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4eabe-2802-4dc5-8a75-1cee4e55e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for the percent of the population with a GED\n",
    "grph_ge_edu = plot_pop_data([col_idx[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558ffd8-94ac-4f75-ae13-5176add64fc2",
   "metadata": {},
   "source": [
    "Another rise in the already high levels of the population with a GED over a high school diploma correlates with that same Covid timing. Likely still indicative of a larger percent of the population pursuing alternative forms of education following the remote learning trends forced onto the system by the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643b57c-1b8f-4bbf-98a1-e35da325ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for the percent of the population with a high school education\n",
    "grph_hs_edu = plot_pop_data([col_idx[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddd546-5319-49a7-a084-faddb4e1c0c4",
   "metadata": {},
   "source": [
    "Interesting how this metric was on the decline even before Covid, and slightly accelerated after. Still, given the preceding graph, it is likely that much of the population that eschews a high school education is pursuing that same qualification through a GED, or some other means such as a profressional education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489b6d4-da05-49ec-9543-2d7d7126f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing for the percent of the population with an associates degree\n",
    "grph_as_edu = plot_pop_data([col_idx[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c49588-9859-45b3-bc8d-16e393c7270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing for the percent of the population with a bachelors degree\n",
    "grph_bd_edu = plot_pop_data([col_idx[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe065d-6d69-4afd-b4c6-49919df90652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing for the percent of the population with a professinal education\n",
    "grph_pr_edu = plot_pop_data([col_idx[7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac87f5-ad7e-43f3-9827-484edca9effe",
   "metadata": {},
   "source": [
    "While telling on its own, the steady rise in professional education (programs like this boot camp, on-the-job training, apprenticeships, etc) does begin putting into perspective a potential national shift in priorities. While earlier graphs show a departure from the emphasis put on a high school education in favor of a GED, this, too, adds credence to the possibility of educations outside the realm of strict academia being the trend of the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8907b-8a96-4620-b194-c3cd559f71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for the percent of the population working in tech-related fields\n",
    "grph_tj_pct = plot_pop_data([col_idx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf880b4-2d5e-4595-9742-3520d1d68e62",
   "metadata": {},
   "source": [
    "Perhaps even more telling, an even sharper rise in the prevelance of tech-related jobs seems to align with the same trends in profressional education. While not a complete picture, given that the tech fields have a larger relationship to the growth in AI and ML technologies and positions, it would be easy to see how the two (the percentage of people working in tech-related jobs and the percentage of people with a professional education) could be directly related. Much as we enrolled in this program to shift into the ML sector, many could be pursuing similar programs to shift into this or other sectors within the fields of technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd0bc7-3ef8-4be0-9085-bc73df9c2b8d",
   "metadata": {},
   "source": [
    "# Slicing for the percent of the population who are unemployed\n",
    "grph_un_pct = plot_pop_data([col_idx[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497512f-1060-4384-82cf-bcfdf5440cd5",
   "metadata": {},
   "source": [
    "The national unemployment numbers have been on a steady decline over the previous decade, and seemed to have relatively stabalized in the years following Covid. Again, while not a complete picture, it is interesting to see how similarly the unemployment rates have decreased in comparrison to the rise in tech-related jobs. It is a possibility that the expansion of tech sectors *and* the increases in available profressional educations could be directly impacting the job market in such a way to help alleviate unemployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d00d9c-78ca-4454-a2ae-f88dbb4e2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing for the percent of the population working in tech-related fields\n",
    "# and with a profressinal education\n",
    "grph_comp_tj_pr = plot_pop_data([col_idx[1],col_idx[7]], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e42487-fd15-49d6-b8b2-3dc03fdb469f",
   "metadata": {},
   "source": [
    "When more directly comparing the trends in tech-related jobs and professional education rates it becomes clearer how closely the two metrics seem to grow. As the two subplots are on slightly different scales, it may appear that the two may have a more positive correlation that they might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4da94-08c9-423b-8c39-340f93712e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max(graph, col_idx):\n",
    "    return {\n",
    "        'min': round(graph[col_idx].min(),2),\n",
    "        'max': round(graph[col_idx].max(),2),\n",
    "        'change_pct': round(graph[col_idx].max() - graph[col_idx].min(), 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401f979-66b6-4ed3-8fe9-d3a11c23f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the minimum and maximum values for `Percent in Tech Fields`\n",
    "min_max_tech = calculate_min_max(grph_comp_tj_pr, col_idx[1])\n",
    "# Calculating the minimum and maximum values for `Percent Professional Education`\n",
    "min_max_edup = calculate_min_max(grph_comp_tj_pr, col_idx[7])\n",
    "# Printing the results\n",
    "print(f'The lowest percentage of people working in tech related fields was: {min_max_tech[\"min\"]}%')\n",
    "print(f'The highest percentage of people working in tech related fields was: {min_max_tech[\"max\"]}%')\n",
    "print(f'This was a growth of: {min_max_tech[\"change_pct\"]}%\\n')\n",
    "print(f'The lowest percentage of people with a profressional education was: {min_max_edup[\"min\"]}%')\n",
    "print(f'The highest percentage of people with a profressional education was: {min_max_edup[\"max\"]}%')\n",
    "print(f'This was a growth of: {min_max_edup[\"change_pct\"]}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a698fae-fd73-48d8-b586-9cb620e21b35",
   "metadata": {},
   "source": [
    "Clearly not as closely related as first impressions might show, there is still definitive growth in both areas. Further analysis on the correlation may be needed, but the conlcusion that an increase in the availablity and prevelance of tech-related jobs and the growing trends in pursuing a professinal education are inherently connected may be a safe one to draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8453f-5c9c-4fcd-9e2d-6250a900e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for the percent of the population working in tech-related fields\n",
    "# and the percent of the population who are unemployed\n",
    "grph_comp_tj_un = plot_pop_data([col_idx[1],col_idx[0]], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aaa74d-5731-4db4-b759-9b8b81faf5db",
   "metadata": {},
   "source": [
    "Similarly as before, a slight difference of scale in these subplots warrants further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b8074-2ea2-4465-bb25-2b2732f3525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the minimum and maximum values for `Percent Unemployed`\n",
    "min_max_unem = calculate_min_max(grph_comp_tj_un, col_idx[0])\n",
    "# Printing the results\n",
    "print(f'The lowest percentage of people who were unemployed was: {min_max_unem[\"min\"]}%')\n",
    "print(f'The highest percentage of people who were unemployed was: {min_max_unem[\"max\"]}%')\n",
    "print(f'This was a decrease of: {min_max_unem[\"change_pct\"]}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50873593-9350-44af-8213-9321027f73b7",
   "metadata": {},
   "source": [
    "Unemployment is a largely complicated metric to examine in any capacity, let alone a simple comparrison to a partially related trend. Many more factors play into the national unemployment numbers, so it would be wholly unfair to say an increase in tech-related jobs is solving unemployment issues across the nation. However, given how closely the trends have grown and decreased, it would be a safe assumption to make that a larger number of available technical job opporunities ***does*** contribute to part of the overall solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f9f02-4789-42e8-806f-04a47527476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grph_comp_4way = plot_pop_data([col_idx[1],col_idx[7], col_idx[0], col_idx[4]], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0dfb2-0b2f-43b8-80cf-db3ec0bd4a25",
   "metadata": {},
   "source": [
    "Similar to the previous two subplots, the addition of a decrease in the percentage of the population with a high school eductaion similarly speaks as an incomplete section of a larger picture. While the trends appear similar at large, there are many more factors at play than the available dataset allows for deeper examination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb30bb-2cd3-47f1-96d4-f935482b5c04",
   "metadata": {},
   "source": [
    "### Preparing Visualiztions\n",
    "\n",
    "Given some of the findings above, the following visualizations on national trends are prepared for the final presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad3c4c-31fe-45bd-8fa6-8cd682efa597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 6))\n",
    "\n",
    "# Plot 'Percent in Tech Fields'\n",
    "axes[0].plot(grph_comp_tj_pr.index, grph_comp_tj_pr['Percent in Tech Fields'], label='Working in Tech Fields', color='blue')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Percentage of US Population')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 'Percent Professional Education'\n",
    "axes[1].plot(grph_comp_tj_pr.index, grph_comp_tj_pr['Percent Professional Education'], label='With a Professional Education', color='red')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Percentage of US Population')\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "#plt.savefig('comp_tech_v_profedu_subplots.png', dpi=300) #Commented out to prevent redundant files\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae034cee-c983-4147-b204-e6c2325fa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a figure and subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 6))\n",
    "\n",
    "# Plotting 'Percent in Tech Fields'\n",
    "axes[0].plot(grph_comp_tj_un.index, grph_comp_tj_un['Percent in Tech Fields'], label='Working in Tech Fields', color='blue')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Percentage of US Population')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plotting 'Percent Unemployed'\n",
    "axes[1].plot(grph_comp_tj_un.index, grph_comp_tj_un['Percent Unemployed'], label='Who Are Unemployed', color='orange')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Percentage of US Population')\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjusting layout and saving the figure\n",
    "plt.tight_layout()\n",
    "#plt.savefig('comp_tech_v_unemp_subplots.png', dpi=300) #Commented out to prevent redundant files\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d1d20-bf2f-43ed-9a42-3cb25a325ca4",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "With the above visualizations prepared, a closer look at the correlations among the various levels of education, unemployment rates, and the growth in tech-related jobs seems justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475187bd-1422-48d5-860c-d6868fac260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing relevant informent from `pop_data` and grouping by `Year` to give\n",
    "# the national averages by year\n",
    "national_avg_pct = pop_data[[\n",
    "                            'Year',\n",
    "                            'Percent Unemployed',\n",
    "                            'Percent in Tech Fields',\n",
    "                            'Percent No Education',\n",
    "                            'Percent High School',\n",
    "                            'Percent GED',\n",
    "                            'Percent Associates',\n",
    "                            'Percent Bachelors',\n",
    "                            'Percent Professional Education'\n",
    "                           ]].groupby('Year').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d45d5-bb3a-4c4e-9dd9-c29e52730b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the correlations among the selected columns\n",
    "national_avg_pct[[\n",
    "                  'Percent in Tech Fields',\n",
    "                  'Percent Unemployed',\n",
    "                  'Percent No Education',\n",
    "                  'Percent High School',\n",
    "                  'Percent GED',\n",
    "                  'Percent Professional Education'\n",
    "                ]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0102a-0497-4cc9-80ec-a140e57c0203",
   "metadata": {},
   "source": [
    "* Percent in Tech Fields and Percent Unemployed:\n",
    "    * Again, not a complete story - seeing as many more variables affect unemployment rates - but such a strong relationship suggests that the trends might continue to behave similarly\n",
    "* Percent in Tech Fields and Percent No Education:\n",
    "    * An interesting closeness, escpecially with the context that the correlation between tech-related jobs and percentage of the population with a GED are *not* as close (discussed below)\n",
    "* Percent in Tech Fields and Percent Percent High School:\n",
    "    * Even closer to a 1:-1 than as seen with unemployment rates, the trends might suggest that those moving away from traditional academia may be looking towards the tech sectors for future employment\n",
    "* Percent in Tech Fields and Percent Percent GED:\n",
    "    * One might expect this to be an inverse of the previous correlation, but the connection between receiving a GED and working in the tech field seems to be significantly lower than with other metrics being explored here\n",
    "    * In point of fact, among the selected columns, the strongest correlation with percentage of the population with a GED seems to be the percent of unemployed people with another near 1:-1 relationshio\n",
    "* Percent in Tech Fields and Percent Percent Professional Education:\n",
    "    * Perhaps the most telling of all is this almost perfectly 1:1 relationship, suggesting an almost difinitive connection between alternative forms of education and the growth of tech-related jobs in our country\n",
    "    * While also not a complete picture, this trend validates the importance of programs such as this boot camp in terms of developing a stronger tech workforce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8620587-ab6d-44b4-8341-975afaeb5197",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Using the slices prepared earlier, multiple sets of data are passed through Prophet to give a 5-year prediction based on the US Census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff72e3-8468-4bd3-b132-df664883d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_prophet_model(model, slice_to_model):\n",
    "    # Fitting the model to the appropriately sliced DataFrame\n",
    "    model.fit(slice_to_model)\n",
    "    # Creating rows for 5 years' worth of predictions\n",
    "    model_future = model.make_future_dataframe(periods=5, freq='YE')\n",
    "    model_forcast = model.predict(model_future)\n",
    "    model.plot(model_forcast)\n",
    "    return {'forecast': model_forcast, 'future': model_future}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848c1bc-ba29-4b60-bd2f-99d319203bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instance for Prophet for `Percent in Tech Fields`\n",
    "m_n_t = Prophet()\n",
    "n_tech_forecast = execute_prophet_model(m_n_t, national_tech)['forecast']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e0bd9-bb9d-4914-aa9e-0090298ebe9d",
   "metadata": {},
   "source": [
    "Prophet paints a clear image of moderate to near-exponential copntinued growth in the tech sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336b997-79fe-4995-9cdd-c4325c67d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instance for Prophet for `Percent Professional Education`\n",
    "m_n_e_p = Prophet()\n",
    "n_prf_forecast = execute_prophet_model(m_n_e_p, national_edu_prf)['forecast']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c0271-cfb6-4b92-9781-bde731a410e4",
   "metadata": {},
   "source": [
    "Similarly, Prophet seems to predict overall growth in the trends for professional education that - at a glance - nearly mirrors the projections for the tech-related jobs as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe544e-7593-42e2-9c4d-05b41a5076ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instance for Prophet for `Percent Associates`\n",
    "m_n_e_a = Prophet()\n",
    "execute_prophet_model(m_n_e_a, national_edu_asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73e5dc-a672-4a2c-91ac-3ebd473e4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instance for Prophet for `Percent High School`\n",
    "m_n_e_h = Prophet()\n",
    "execute_prophet_model(m_n_e_h, national_edu_hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94b91b-983c-45ea-8735-161c5df3b333",
   "metadata": {},
   "source": [
    "Unsure as to why such drastic spikes came from the predictive model. This could be due to using a rolling 5-year average, meaning alternative, cleaner data may be required for deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88339ad-0972-4089-b7fc-842375bb8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the appropriately sliced DataFrame\n",
    "m_n_e_g = Prophet()\n",
    "execute_prophet_model(m_n_e_g, national_edu_ged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b18c9-2ce9-4aab-b6f3-0de2c0fcddb5",
   "metadata": {},
   "source": [
    "Same as with `Percent High School`, the spikes in this model indicate alternative data might be needed for closer examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7953c74-247b-4713-bbca-a5e86971f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instance for Prophet for `Percent Unemployed`\n",
    "m_n_u = Prophet()\n",
    "n_unemp_forecast = execute_prophet_model(m_n_u, national_unemp)['forecast']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ffaf1-c214-4e54-92e0-bd002dd7645c",
   "metadata": {},
   "source": [
    "While also a little choppy in presentaion, the overall trend in this prediction on unemployment rates does seem to align with the provided values from the US Census data. Still complicated by variables beyond the scope of this analysis, Prophet's predictions do speak to a potential low in unemployment rates for the nation. Such a prediciton is made all the most interesting when compared to the plot for `Percent in Tech Fields`. Again, tech jobs are only part of the overall contributions to efforts being made through other avenues to lower unemployment numbers, but a healthy relationship between the growth of one and the decline of the other does seem to be present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d7122-d72e-4fa8-b879-643653bdc300",
   "metadata": {},
   "source": [
    "While also a little choppy in presentaion, the overall trend in this prediction on unemployment rates does seem to align with the provided values from the US Census data. Still complicated by variables beyond the scope of this analysis, Prophet's predictions do speak to a potential low in unemployment rates for the nation. Such a prediciton is made all the most interesting when compared to the plot for `Percent in Tech Fields`. Again, tech jobs are only part of the overall contributions to efforts being made through other avenues to lower unemployment numbers, but a healthy relationship between the growth of one and the decline of the other does seem to be present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b07120-ab73-42c0-894b-b0e4cd1e8205",
   "metadata": {},
   "source": [
    "### Predictions (A Closer Look)\n",
    "\n",
    "Taking a deeper look at the predictions Prophet made for `Percent in Tech Fields` and `Percent Unemployed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18604456-005a-4d0a-9890-bc0bbff1f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a closer look at the 5-year prediction ranges made for `Percent in Tech Fields`\n",
    "n_tech_forecast[['yhat', 'yhat_lower', 'yhat_upper']].iloc[-5:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417fd0b-0308-4b2d-bfb4-fa2554cdb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the ranges of predictions at the end of the 5 years for `Percent in Tech Fields`\n",
    "predict_tech_min = n_tech_forecast['yhat_lower'].iloc[-1]\n",
    "predict_tech_mid = n_tech_forecast['yhat'].iloc[-1]\n",
    "predict_tech_max = n_tech_forecast['yhat_upper'].iloc[-1]\n",
    "\n",
    "# Printing predictions\n",
    "print(f'The lowest prediction for percentage of the population working in tech-related jobs is: {round(predict_tech_min, 2)}%')\n",
    "print(f'The middle prediction for percentage of the population working in tech-related jobs is: {round(predict_tech_mid, 2)}%')\n",
    "print(f'The highest prediction for percentage of the population working in tech-related jobs is: {round(predict_tech_max, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3259d-6f4d-4efd-a738-af97720206d8",
   "metadata": {},
   "source": [
    "All outcomes seem plausible based on the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a47b3-e607-40b7-97b7-84b0c1e323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a closer look at the 5-year prediction ranges made for `Percent Unemployed`\n",
    "n_unemp_forecast[['yhat', 'yhat_lower', 'yhat_upper']].iloc[-5:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d55664-4ede-4160-b5ef-2bbdbaedc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the ranges of predictions at the end of the 5 years for `Percent Unemployed`\n",
    "predict_unem_min = n_unemp_forecast['yhat_lower'].iloc[-1]\n",
    "predict_unem_mid = n_unemp_forecast['yhat'].iloc[-1]\n",
    "predict_unem_max = n_unemp_forecast['yhat_upper'].iloc[-1]\n",
    "\n",
    "# Printing predictions\n",
    "print(f'The lowest prediction for the unemployed percentage of the population is: {round(predict_unem_min, 2)}%')\n",
    "print(f'The middle prediction for the unemployed percentage of the population is: {round(predict_unem_mid, 2)}%')\n",
    "print(f'The highest prediction for the unemployed percentage of the population is: {round(predict_unem_max, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf720ba-1339-4e44-9b26-1aa1ce22dbf5",
   "metadata": {},
   "source": [
    "Seeing as the lowest rate for national unemployment to date has been 3.6% (__[as per Statista](https://www.statista.com/statistics/193290/unemployment-rate-in-the-usa-since-1990/#:~:text=By%20the%20end%20of%202022,Both%20are%20seasonally%20adjusted.)__), none of these predictions feel attainable. An interesting exercise in thought, but the subject may be far beyond Prophet's abilities to predict - especially with the limitted data available for this exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647470e-9f37-4ce0-9fed-05ed0c98d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating subplots for earlier forecasts\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "\n",
    "# Plotting forecast for `n_tech_forecast`\n",
    "m_n_t.plot(n_tech_forecast, ax=axes[0])\n",
    "axes[0].set_title('Percentage Employed in Tech Fields')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Percentage of Population')\n",
    "\n",
    "# Plotting forecast for `n_prf_forecast`\n",
    "m_n_e_p.plot(n_prf_forecast, ax=axes[1])\n",
    "axes[1].set_title('Percentage Pursuing Professional Education')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Percentage of Population')\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.suptitle('Prophet Predictions', x=0.5, y=0.98, ha='center', fontsize='x-large')\n",
    "\n",
    "# Adjusting layout and saving the figure\n",
    "plt.tight_layout()\n",
    "#plt.savefig('comp_tech_v_profedu_predict.png', dpi=300) #Commented out to prevent redundant files\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e48ea4-5a9d-4698-b80d-c762c5905e97",
   "metadata": {},
   "source": [
    "# US Census Summary\n",
    "\n",
    "After more closely examining the US Census data for trends and patterns in how tech-related jobs correlate to education and employment rates in our country, the following can be said;\n",
    "\n",
    "#### Education Trends\n",
    "\n",
    "There is a definite trend over the past decade in a departure from traditional academia. While the percentage of the US population with more advanced degrees (eg; associates, bachelors, and masters) seems to be on a steady increase, other metrics have seen far more drastic changes. The percentage of the population without a high school education (eg; no education or GED in lieu of a traditional high school diploma) have seem massive surges in the years examined. Given the effect of Covid on the American education system, there are likely factors beyond the scope of the available data at play. That said, both the percentage of the population no education *and* the percentage of the population with a high school degree share similar and inverse correlations to the percentage of the population working in tech-related fields (1:0.935 and 1:-0.978, respectively).\n",
    "\n",
    "Perhaps the strongest case can be made for the connection between tech-related jobs and professional education. With a 1:0.997 correlation (the single strongest found in this dataset), a clear image is painted of the importance of programs (such as this boot camp) for training the skills necessary to work in such fields. With a growing number of the population seeking alternative methods for education, and a large demand for highly-specific skillsets, a national trend moving towards more customizable, directly-applicatble learning makes sense. And, with AI and ML job demands being a subset of the overall tech sector, the growth of such positions would inherently help to drive further demand for both the jobs and programs to train the skills necessary for them.\n",
    "\n",
    "#### Unemployment Trends\n",
    "\n",
    "When compared to the percentage of the population working in tech-related fields, the unemployment percentage appears somewhat related (a 1:-0.908 correlation). However, these numbers are being generated by a rolling 5-year national average, and are not indicative of all the variables playing into the unemployment rates. There is a case to be made for the growth in tech sectors helping to combat unemployment rates, but it must be acknowledged that the data used in this analysis does not -  and cannot - tell the whole story. Far more data, from more varied sources, would be necessary to draw more concrete conclusions about the connections between tech jobs and unemployment.\n",
    "\n",
    "##### Future Analysis\n",
    "\n",
    "Much of the analysis done with the US Census data is limited in scope. Comprised of 10 years' worth of rolling 5-year estimates, there is room for both interpretation and improvement. Ideally, if further research is pursued, using the Census' ACS1 might be prefereble to the ACS5 dataset used here. Additional supplementary data would help to further strengthen any correlations drawn - specifically data relating to how tech-related jobs are affecting layoffs and unemployment rates more directly. Furthermore, a more granular subset of tech-related jobs (those directly connected to AI and ML jobs) would helpe more closesly relate the findings to the overall question of how such jobs interact with other aspects of employment and education. Given the methodology of how the US Census Bureau tracks fields of employment, this would absolutely necessetate seeking alternative data sources. However, most applicable sources remain behind paywalls and are inaccessable for the purposes of this project. With access to a wider variety of information, though, more concerete conclusions may be found with further exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "ai_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
